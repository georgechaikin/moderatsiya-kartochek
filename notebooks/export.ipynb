{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fb6935b-56cf-4af2-9496-27debec25c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import openvino as ov\n",
    "from transformers import BlipProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8180d0dc-48a5-495e-bb81-4f1751e9b85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss, BCEWithLogitsLoss\n",
    "from transformers import CLIPForImageClassification, Trainer, TrainingArguments\n",
    "from transformers import BlipConfig, BlipModel, BlipVisionModel, BlipPreTrainedModel\n",
    "from transformers.modeling_outputs import ImageClassifierOutput\n",
    "\n",
    "\n",
    "# Класс наподобие CLIPForImageClassification\n",
    "class BlipForImageClassification(BlipPreTrainedModel):\n",
    "    \"\"\"\n",
    "    Классификатор с BLIP Vision Encoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: BlipConfig) -> None:\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        vision_model = BlipVisionModel._from_config(\n",
    "            config.vision_config, attn_implementation=config._attn_implementation\n",
    "        )\n",
    "        self.vision_model = vision_model\n",
    "\n",
    "        # Classifier head\n",
    "        self.classifier = (\n",
    "            nn.Linear(config.vision_config.hidden_size, config.num_labels) if config.num_labels > 0 else nn.Identity()\n",
    "        )\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[tuple, ImageClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.vision_model(\n",
    "            pixel_values,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        # average pool the patch tokens\n",
    "        sequence_output = torch.mean(sequence_output[:, 1:, :], dim=1)\n",
    "        # apply classifier\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # move labels to correct device to enable model parallelism\n",
    "            labels = labels.to(logits.device)\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return ImageClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "904f5af6-9e7e-40e3-aa4f-d32622ca0b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "core = ov.Core()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ef5360b-01ba-4002-9c3d-ac52f91470ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "@dataclass\n",
    "class ImageDataset(Dataset):\n",
    "    dataset: Dataset\n",
    "    processor: BlipProcessor\n",
    "    positive_labels:List[str]\n",
    "\n",
    "    def __len__(self)->int:\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx: int)->Tuple:\n",
    "        image = self.dataset[idx]['image'].convert(\"RGB\")\n",
    "        inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "        inputs['pixel_values'] = inputs['pixel_values'].squeeze(0)\n",
    "        label = self.dataset[idx]['label']\n",
    "        label_string = self.dataset.features['label'].int2str(label)\n",
    "        label = 1 if label_string in self.positive_labels else 0\n",
    "        inputs['label'] = label\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64eac337-7cce-4bdd-bc4c-fa00dbc71773",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = \"Salesforce/blip-image-captioning-large\"\n",
    "model_weights = Path(r\"..\\data\\models\\blip-large-probe-2\\checkpoint-16550\")\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(model_name)\n",
    "model = BlipForImageClassification.from_pretrained(model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c600fac7-5313-468f-94d6-2f9f2291afff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loading...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de86c97c117f4109a5297de5e3e5fc95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/6602 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': Image(mode=None, decode=True, id=None), 'label': ClassLabel(names=['cigs', 'other', 'pipes', 'roll_cigs', 'smoking'], id=None)}\n",
      "Convert dataset for specific model...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Dataset loading...\")\n",
    "data_dir = Path('../data')\n",
    "img_ds = load_dataset(\"imagefolder\", data_dir=data_dir, split=\"train\")\n",
    "print(img_ds.features)\n",
    "print(\"Convert dataset for specific model...\")\n",
    "positive_classes = list(filter(lambda name: name!='other', img_ds.features['label'].names))\n",
    "ds = ImageDataset(img_ds, processor, positive_classes)\n",
    "batch_loader = torch.utils.data.DataLoader(ds, batch_size=4)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3340b0-364e-4805-a729-db952eff2e33",
   "metadata": {},
   "source": [
    "## Image Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30a3a32a-c704-4810-bd5c-ea52335c3306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Classifier will be loaded from ..\\..\\data\\models\\openvino\\blip_image_classifier.xml\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "import openvino as ov\n",
    "\n",
    "IMAGE_CLASSIFIER_OV = Path(\"../data/models/openvino/blip_image_classifier.xml\")\n",
    "image_classifier = model\n",
    "image_classifier.eval()\n",
    "\n",
    "# check that model works and save it outputs for reusage as text encoder input\n",
    "# inputs = ds[0]\n",
    "pixel_values = ds[0]['pixel_values'].unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    outputs = image_classifier(pixel_values)\n",
    "\n",
    "# if openvino model does not exist, convert it to IR\n",
    "if not IMAGE_CLASSIFIER_OV.exists():\n",
    "\n",
    "    # export pytorch model to ov.Model\n",
    "    with torch.no_grad():\n",
    "        ov_image_classifier = ov.convert_model(image_classifier, example_input=pixel_values)\n",
    "    # save model on disk for next usages\n",
    "    ov.save_model(ov_image_classifier, IMAGE_CLASSIFIER_OV)\n",
    "    print(f\"Image Classifier model successfuly converted and saved to {IMAGE_CLASSIFIER_OV}\")\n",
    "else:\n",
    "    print(f\"Image Classifier will be loaded from {IMAGE_CLASSIFIER_OV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef2ee3d9-fb52-4c08-99ce-b7ae8f5a3a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ov_image_classifier = core.compile_model(IMAGE_CLASSIFIER_OV, \"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6ffa149-0be3-49ef-9894-4ca199f1c0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8078680038452148\n"
     ]
    }
   ],
   "source": [
    "batch_loader = torch.utils.data.DataLoader(ds, batch_size=1)\n",
    "for batch in batch_loader:\n",
    "    break\n",
    "\n",
    "pixel_values = batch['pixel_values'].numpy()\n",
    "start_time = time.time()\n",
    "logits = ov_image_classifier(pixel_values)[ov_image_classifier.output(0)]\n",
    "end_time = time.time()\n",
    "print(end_time - start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
